{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic PCA Tutorial\n",
    "This tutorial will demonstrate Probabilistic PCA, a  factor analysis technique. \n",
    "\n",
    "Maths and notation following [Machine Learning: A Probabilistic Perspective](https://www.amazon.com/gp/product/0262018020).\n",
    "\n",
    "## Installation\n",
    "Follow the instrallation instructions in the [README](../../README.md) file to get setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Probabalistic Modeling Introduction\n",
    "\n",
    "Probabilistic Models can be\n",
    "categorized into directed graphical models (DGM, Bayes Net) and undirected\n",
    "graphical models (UGM). Most popular probabilistic models\n",
    "are DGMs, so MXFusion will only support the definition of\n",
    "DGMs unless there is a strong customer need of UGMs in future.\n",
    "\n",
    "A DGM can be fully defined using 3 basic components: deterministic functions,\n",
    "probabilistic distributions, and random variables. We show the interface for\n",
    "defining a model using each of the three components below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets import the basic libraries we'll need to train our model and visualize some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mxfusion as mf\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "We'll take as our function to learn components of the [log spiral function](https://en.wikipedia.org/wiki/Logarithmic_spiral) because it's 2-dimensional and easy to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_spiral(a,b,t):\n",
    "    x = a * np.exp(b*t) * np.cos(t)\n",
    "    y = a * np.exp(b*t) * np.sin(t)\n",
    "    return np.vstack([x,y]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parameterize the function with 100 data points and plot the resulting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = 100\n",
    "K = 2\n",
    "\n",
    "a = 1\n",
    "b = 0.1\n",
    "t = np.linspace(0,6*np.pi,N)\n",
    "r = log_spiral(a,b,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x112ebbef0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112e625f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r[:,0], r[:,1],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now our project our $K$ dimensional ```r``` into a high-dimensional $D$ space using a random matrix of random weights $W$. Now that ```r``` is embedded in a $D$ dimensional space the goal of PPCA will be to recover ```r``` in it's original low-dimensional $K$ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(K,N)\n",
    "x_train = np.dot(r,w) + np.random.randn(N,N) * 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# new_r = pca.fit_transform(r_high)\n",
    "# plt.plot(new_r[:,0], new_r[:,1],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the higher dimensional data manually by changing ```dim1``` and ```dim2``` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112e62208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim1 = 79\n",
    "dim2 = 11\n",
    "plt.scatter(x_train[:,dim1], x_train[:,dim2], color='blue', alpha=0.1)\n",
    "plt.axis([-10, 10, -10, 10])\n",
    "plt.title(\"Simulated data set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXFusion Model Definition\n",
    "Import MXFusion and MXNet modelling components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.models import Model\n",
    "import mxnet.gluon.nn as nn\n",
    "from mxfusion.components import Variable\n",
    "from mxfusion.components.variables import PositiveTransformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary data structure in MXFusion is the Model. Models hold ModelComponents, such as Variables, Distributions, and Functions which are the what define a probabilistic model. \n",
    "\n",
    "The model we'll be defining for PPCA is:\n",
    "\n",
    "$p(z)$ ~ $N(\\mathbf{\\mu}, \\mathbf{\\Sigma)}$\n",
    "\n",
    "$p(x | z,\\theta)$ ~ $N(\\mathbf{Wz} + \\mu, \\Psi)$\n",
    "\n",
    "where:\n",
    "\n",
    "$z \\in \\mathbb{R}^{N x K}, \\mathbf{\\mu} \\in \\mathbb{R}^K, \\mathbf{\\Sigma} \\in \\mathbb{R}^{NxKxK}, x \\in \\mathbb{R}^{NxD}$\n",
    "\n",
    "$\\Psi \\in \\mathbb{R}^{NxDxD}, \\Psi = [\\Psi_0, \\dots, \\Psi_N], \\Psi_i = \\sigma^2\\mathbf{I}$\n",
    "\n",
    "$z$ here is our latent variable of interest, $x$ is the observed data, and all other variables are parameters or constants of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create an MXFusion Model object to build our PPCA model on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attach ```Variable``` objects to our model to collect them in a centralized place. Internally, these are organized into a factor graph which is used during Inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.w = Variable(shape=(K,D), initial_value=mx.nd.array(np.random.randn(K,D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the mean of $x$'s distribution is composed of the dot product of $z$ and $W$, we need to create a dot product function. First we create a dot product function in MXNet and then wrap the function into MXFusion using the MXFusionGluonFunction class. ```m.dot``` can then be called like a normal python function and will apply to the variables it is called on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = nn.HybridLambda(function='dot')\n",
    "m.dot = mf.functions.MXFusionGluonFunction(dot, num_outputs=1, broadcastable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define ```m.z``` which has an identity matrix covariance, ```cov```, and zero mean.\n",
    "\n",
    "```m.z``` and ```sigma_2``` are then used to define ```m.x```.\n",
    "\n",
    "Note that both ```sigma_2``` and ```cov``` will be added implicitly into the ```Model``` because they are inputs to ```m.x```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = mx.nd.broadcast_to(mx.nd.expand_dims(mx.nd.array(np.eye(K,K)), 0),shape=(N,K,K))\n",
    "m.z = mf.distributions.MultivariateNormal.define_variable(mean=mx.nd.zeros(shape=(N,K)), covariance=cov, shape=(N,K))\n",
    "sigma_2 = Variable(shape=(1,), transformation=PositiveTransformation())\n",
    "m.x = mf.distributions.Normal.define_variable(mean=m.dot(m.z, m.w), variance=sigma_2, shape=(N,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    m = Model()\n",
    "    m.w = Variable(shape=(K,D), initial_value=mx.nd.array(np.random.randn(K,D)))\n",
    "    dot = nn.HybridLambda(function='dot')\n",
    "    m.dot = mf.functions.MXFusionGluonFunction(dot, num_outputs=1, broadcastable=False)\n",
    "    cov = mx.nd.broadcast_to(mx.nd.expand_dims(mx.nd.array(np.eye(K,K)), 0),shape=(N,K,K))\n",
    "    m.z = mf.distributions.MultivariateNormal.define_variable(mean=mx.nd.zeros(shape=(N,K)), covariance=cov, shape=(N,K))\n",
    "    sigma_2 = Variable(shape=(1,), transformation=PositiveTransformation())\n",
    "    m.x = mf.distributions.Normal.define_variable(mean=m.dot(m.z, m.w), variance=sigma_2, shape=(N,D))\n",
    "    return m\n",
    "m = make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Definition\n",
    "\n",
    "Now that we have our model, we need to define a posterior with parameters for the inference algorithm to optimize. When constructing a Posterior, we pass in the Model it is defined over and ModelComponent's from the original Model are accessible and visible in the Posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix must continue to be positive definite throughout the optimization process in order to succeed in the Cholesky decomposition when drawing samples or computing the log pdf of ```q.z```. To satisfy this, we pass the covariance matrix parameters through a Gluon function that forces it into a Symmetric matrix which for suitable initialization values should maintain positive definite-ness throughout the optimization procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference.score_function import ScoreFunctionInference, ScoreFunctionRBCVInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxfusion.inference import BatchInferenceLoop, GradBasedInference, StochasticVariationalInference\n",
    "class SymmetricMatrix(mx.gluon.HybridBlock):\n",
    "    def hybrid_forward(self, F, x, *args, **kwargs):\n",
    "        return F.sum((F.expand_dims(x, 3)*F.expand_dims(x, 2)), axis=-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this model has an analytical solution, we will run Variational Inference to find the posterior to demonstrate inference in a setting where the answer is known. \n",
    "\n",
    "We place a multivariate normal prior over $z$ because that is $z$'s prior in the model and we don't need to approximate anything in this case. Because the form we're optimizing over is the true model, the optimization is convex and will always converge to the same answer given by classical PCA given enough iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_post(m):\n",
    "    q = mf.models.Posterior(m)\n",
    "    sym = mf.components.functions.MXFusionGluonFunction(SymmetricMatrix(), num_outputs=1, broadcastable=False)\n",
    "    cov = Variable(shape=(N,K,K), initial_value=mx.nd.broadcast_to(mx.nd.expand_dims(mx.nd.array(np.eye(K,K) * 1e-2), 0),shape=(N,K,K)))\n",
    "    q.post_cov = sym(cov)\n",
    "    q.post_mean = Variable(shape=(N,K), initial_value=mx.nd.array(np.random.randn(N,K)))\n",
    "    q.z.set_prior(mf.distributions.MultivariateNormal(mean=q.post_mean, covariance=q.post_cov))\n",
    "    return q\n",
    "q = make_post(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take our posterior and model, along with an observation pattern (in our case only ```m.x``` is observed) and create an inference algorithm. This inference algorithm is combined with a gradient loop to create the Inference method ```infr```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(inf_type, num_samples=100):\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    mx.random.seed(0)\n",
    "    m = make_model()\n",
    "    q = make_post(m)\n",
    "    observed = [m.x]\n",
    "    if inf_type == 'sf':\n",
    "        alg = ScoreFunctionInference(num_samples=num_samples, model=m, posterior=q, observed=observed)\n",
    "    elif inf_type == 'rb':\n",
    "        alg = ScoreFunctionRBCVInference(num_samples=num_samples, model=m, posterior=q, observed=observed)\n",
    "    else:\n",
    "        alg = StochasticVariationalInference(num_samples=num_samples, model=m, posterior=q, observed=observed)\n",
    "    infr = GradBasedInference(inference_algorithm=alg,  grad_loop=BatchInferenceLoop())\n",
    "    infr.initialize(x=mx.nd.array(x_train))\n",
    "    infr.run(max_iter=3, learning_rate=1e-2, x=mx.nd.array(x_train), verbose=False)\n",
    "    return infr, q.post_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_x_z \n",
      "[-90605.29]\n",
      "<NDArray 1 @cpu(0)>\n",
      "q_z_lambda \n",
      "[636.19214]\n",
      "<NDArray 1 @cpu(0)>\n",
      "NORMAL gradient_lambda \n",
      "[-58047116.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "\n",
      "\n",
      "p_x_z \n",
      "[-89612.41]\n",
      "<NDArray 1 @cpu(0)>\n",
      "q_z_lambda \n",
      "[526.2015]\n",
      "<NDArray 1 @cpu(0)>\n",
      "NORMAL gradient_lambda \n",
      "[-47431068.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "\n",
      "\n",
      "p_x_z \n",
      "[-88634.414]\n",
      "<NDArray 1 @cpu(0)>\n",
      "q_z_lambda \n",
      "[421.47275]\n",
      "<NDArray 1 @cpu(0)>\n",
      "NORMAL gradient_lambda \n",
      "[-37534630.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "\n",
      "\n",
      "q_i \n",
      "[636.19214]\n",
      "<NDArray 1 @cpu(0)>\n",
      "p_i \n",
      "[-90605.29]\n",
      "<NDArray 1 @cpu(0)>\n",
      "f_i \n",
      "[-58047116.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "grad_i \n",
      "[-58047116.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "RB gradient_lambda \n",
      "[-58047116.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[16:04:48] src/imperative/imperative_utils.cc:58: Check failed: !ndinputs.back()->is_none() node_3410 0\n\nStack trace returned 7 entries:\n[bt] (0) 0   libmxnet.so                         0x0000000106552b90 libmxnet.so + 15248\n[bt] (1) 1   libmxnet.so                         0x000000010655293f libmxnet.so + 14655\n[bt] (2) 2   libmxnet.so                         0x0000000107a9bd64 MXNDListFree + 587044\n[bt] (3) 3   libmxnet.so                         0x0000000107a917d3 MXNDListFree + 544659\n[bt] (4) 4   libmxnet.so                         0x00000001079e5e1e MXAutogradBackwardEx + 1022\n[bt] (5) 5   _ctypes.cpython-36m-darwin.so       0x00000001056ff02f ffi_call_unix64 + 79\n[bt] (6) 6   ???                                 0x00007fff5c1f3900 0x0 + 140734738938112\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1dce07f31eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msf_infr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrb_infr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrb_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msvi_infr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvi_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'svi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-1eaa1debbcfe>\u001b[0m in \u001b[0;36mget_grad\u001b[0;34m(inf_type, num_samples)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradBasedInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference_algorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mgrad_loop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBatchInferenceLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0minfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0minfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minfr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/github_mxfusion/MXFusion/mxfusion/inference/grad_based_inference.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, learning_rate, max_iter, verbose, **kw)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0minfr_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmxnet_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             learning_rate=learning_rate, max_iter=max_iter, verbose=verbose)\n\u001b[0m",
      "\u001b[0;32m~/workspace/github_mxfusion/MXFusion/mxfusion/inference/batch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, infr_executor, data, param_dict, ctx, optimizer, learning_rate, max_iter, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_for_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfr_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss_for_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rIteration {} logL: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/jupyter3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad, retain_graph, train_mode)\u001b[0m\n\u001b[1;32m   2190\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m             ctypes.c_void_p(0)))\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtostype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/jupyter3/lib/python3.6/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \"\"\"\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [16:04:48] src/imperative/imperative_utils.cc:58: Check failed: !ndinputs.back()->is_none() node_3410 0\n\nStack trace returned 7 entries:\n[bt] (0) 0   libmxnet.so                         0x0000000106552b90 libmxnet.so + 15248\n[bt] (1) 1   libmxnet.so                         0x000000010655293f libmxnet.so + 14655\n[bt] (2) 2   libmxnet.so                         0x0000000107a9bd64 MXNDListFree + 587044\n[bt] (3) 3   libmxnet.so                         0x0000000107a917d3 MXNDListFree + 544659\n[bt] (4) 4   libmxnet.so                         0x00000001079e5e1e MXAutogradBackwardEx + 1022\n[bt] (5) 5   _ctypes.cpython-36m-darwin.so       0x00000001056ff02f ffi_call_unix64 + 79\n[bt] (6) 6   ???                                 0x00007fff5c1f3900 0x0 + 140734738938112\n\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100\n",
    "sf_infr, sf_mean = get_grad('sf', num_samples)\n",
    "rb_infr, rb_mean = get_grad('rb', num_samples)\n",
    "svi_infr, svi_mean = get_grad('svi', num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_np = sf_infr.params[sf_mean].grad.asnumpy()\n",
    "rb_np = rb_infr.params[rb_mean].grad.asnumpy()\n",
    "normal_np = svi_infr.params[svi_mean].grad.asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_np[:5], rb_np[:5], normal_np[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.abs(sf_np)))\n",
    "print(np.sum(np.abs(rb_np)))\n",
    "print(np.sum(np.abs(normal_np)))\n",
    "\n",
    "print(np.sum(np.abs(sf_np - normal_np)))\n",
    "print(np.sum(np.abs(rb_np - normal_np)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# mx.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed = [m.x]\n",
    "# alg = StochasticVariationalInference(num_samples=10, model=m, posterior=q, observed=observed)\n",
    "# infr = GradBasedInference(inference_algorithm=alg,  grad_loop=BatchInferenceLoop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alg_sf = ScoreFunctionInference(num_samples=10, model=m, posterior=q, observed=observed)\n",
    "# infr_sf = GradBasedInference(inference_algorithm=alg,  grad_loop=BatchInferenceLoop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference method is then initialized with our training data and we run optimiziation for a while until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infr.initialize(x=mx.nd.array(x_train))\n",
    "# infr_sf.initialize(x=mx.nd.array(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# infr.run(max_iter=1, learning_rate=1e-2, x=mx.nd.array(x_train))\n",
    "# infr_sf.run(max_iter=1, learning_rate=1e-2, x=mx.nd.array(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = infr.params[q.post_mean].grad\n",
    "# infr_sf.params[q.post_mean].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training completes, we retrieve the posterior mean (our trained representation for $\\mathbf{Wz} + \\mu$) from the inference method and plot it. \n",
    "As shown, the plot recovers (up to rotation) the original 2D data quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_z_mean = infr.params[q.z.factor.mean].asnumpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
